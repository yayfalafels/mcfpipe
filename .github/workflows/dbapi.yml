---
name: Database API

on:
  workflow_dispatch:
  push:
    paths:
      - '.github/workflows/dbapi.yml'
      - 'aws/cloudformation/db_api_base.yaml'
      - 'aws/cloudformation/db_api_stack.yaml'
      - 'storage/db_schema.json'

jobs:
  deploy-dbapi:
    name: Deploy DB API
    runs-on: ubuntu-latest

    env:
      CONFIG_PATH: setup/config.env
      STACK_NAME: mcfpipe-dbapi
      TAG_ROLE: Bridges
      TAG_PROJECT: mcfpipe
      CF_TEMPLATE_DIR: aws/cloudformation
      BASE_TEMPLATE_FILE: db_api_base.yaml
      STACK_TEMPLATE_FILE: db_api_stack.yaml
      LAMBDA_SC_S3_DIR: apps/jobdb
      LAMBDA_SC_ZIP: lambda_sc_jobdb.zip
      STORAGE_S3_DIR: storage
      DB_SCHEMA_JSON: db_schema.json
      ARTIFACTS_JSON: db_api.json
      NETWORK_CONFIG_JSON: network_config.json
      NETWORK_CONFIG_S3_PATH: aws/network
      DBAPI_APP_DIR: jobdb
      DBAPI_APP: jobdb
      TEMPLATE_CONSTRUCTOR: cf_template_constructor.py

    steps:
      - name: Init Run Status
        id: initialize_run_status
        run: echo "DBAPI_SUCCESS=true" >> $GITHUB_ENV

      - name: Checkout repository
        id: repo_checkout
        uses: actions/checkout@v3

      - name: Load environment config
        id: env_var_set
        run: cat $CONFIG_PATH >> $GITHUB_ENV

      - name: Configure AWS credentials
        id: aws_config
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Download and parse network config
        id: network_config
        shell: bash
        run: |
            set -euo pipefail
            aws s3 cp "s3://$S3_BUCKET/$NETWORK_CONFIG_S3_PATH/$NETWORK_CONFIG_JSON" $NETWORK_CONFIG_JSON
            JQ='map({(.OutputKey): .OutputValue})|add'
            echo "VPC_ID=$(jq -r "$JQ".VpcId $NETWORK_CONFIG_JSON)" >> "$GITHUB_ENV"
            echo "PRIVATE_SUBNET=$(jq -r "$JQ".PrivateSubnetId $NETWORK_CONFIG_JSON)" >> "$GITHUB_ENV"
            echo "DB_LAMBDA_SG=$(jq -r "$JQ".SGPrivate $NETWORK_CONFIG_JSON)" >> "$GITHUB_ENV"

      - name: Upload DB schema to S3
        id: schema_load
        run: aws s3 cp $STORAGE_S3_DIR/$DB_SCHEMA_JSON \
          s3://$S3_BUCKET/$STORAGE_S3_DIR/$DB_SCHEMA_JSON

      - name: Generate CloudFormation template
        id: cf_template_generate
        run: |
          pip install -r $DBAPI_APP_DIR/requirements.txt >/dev/null
          python $DBAPI_APP_DIR/$TEMPLATE_CONSTRUCTOR \
            $STORAGE_S3_DIR/$DB_SCHEMA_JSON \
            $CF_TEMPLATE_DIR/$BASE_TEMPLATE_FILE \
            $CF_TEMPLATE_DIR/$STACK_TEMPLATE_FILE

      - name: Package Lambda handler
        id: zip_lambda_handler
        run: |
          zip -r $LAMBDA_SC_ZIP $DBAPI_APP_DIR/lambda_handler.py $DBAPI_APP_DIR/$DBAPI_APP >/dev/null
          aws s3 cp $LAMBDA_SC_ZIP \
            s3://$S3_BUCKET/$LAMBDA_SC_S3_DIR/$LAMBDA_SC_ZIP

      - name: Deploy CloudFormation Stack
        id: stack_deploy
        run: |
          aws cloudformation deploy \
            --template-file $CF_TEMPLATE_DIR/$STACK_TEMPLATE_FILE \
            --stack-name $STACK_NAME \
            --capabilities CAPABILITY_NAMED_IAM \
            --parameter-overrides \
              VpcId=$VPC_ID \
              PrivateSubnetIds=$PRIVATE_SUBNET \
              S3Bucket=$S3_BUCKET \
              LambdaSourceCodeS3=$LAMBDA_SC_S3_DIR/$LAMBDA_SC_ZIP \
              DBSchemaS3=$STORAGE_S3_DIR/$DB_SCHEMA_JSON \
              DBLambdaSG=$DB_LAMBDA_SG \
            --tags \
              role=$TAG_ROLE \
              project=$TAG_PROJECT

      - name: Get stack outputs and export to runner env
        id: stack_artifacts
        run: |
          OUTPUTS=$(aws cloudformation describe-stacks \
            --stack-name $STACK_NAME \
            --query "Stacks[0].Outputs" \
            --output json)
          echo "$OUTPUTS" > $ARTIFACTS_JSON
          DB_URL=$(echo $OUTPUTS | jq -r \
            '.[] | select(.OutputKey==\"DB_API_URL\") | .OutputValue')
          echo "DB_API_URL=$DB_URL" >> $GITHUB_ENV

      - name: Upload stack outputs to S3
        id: artifacts_upload
        run: aws s3 cp $ARTIFACTS_JSON \
          s3://$S3_BUCKET/$STORAGE_S3_DIR/$ARTIFACTS_JSON

      - name: Fail workflow if build failed
        if: env.DBAPI_SUCCESS == 'false'
        run: |
          echo "DB API deploy failed"
          exit 1
